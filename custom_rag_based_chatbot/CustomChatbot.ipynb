{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbbce4d9-1f49-49d6-b957-e2fe63537ecb",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import common items and setting up OpenAI api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572a8bb8-4616-4c17-a6cd-1d1cf6461e06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing open AI and also initiating client with api_key\n",
    "import os\n",
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "  api_key=\"\", #TO FILL IN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eddaf3-9a22-4985-9f18-82771b6d39e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Testcases: Using original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719911c-b0b2-4212-b689-7186c76fa32c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Using original model to answer cricket player stats\")\n",
    "print(\"***************************************************\")\n",
    "\n",
    "# Testcase#1: Testing with cricket player stats from 2023 based on original model response\n",
    "dhoni_stats_2023_question = \"\"\"\n",
    "Question: \"What are Dhoni stats in 2023?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt = dhoni_stats_2023_question,\n",
    "    max_tokens = 1000,\n",
    ")\n",
    "dhoni_stats_2023_answer = response.choices[0].text.strip()\n",
    "print(\"Question: What are Dhoni stats in 2023?\")\n",
    "print(f\"Answer: {dhoni_stats_2023_answer}\\n\\n\")\n",
    "\n",
    "# Testcase#2: Testing with cricket player stats from 2023 based on original model response\n",
    "rahane_matches_2023_question = \"\"\"\n",
    "Question: \"How many matches Ruturaj played in 2023?\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.completions.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt = rahane_matches_2023_question,\n",
    "    max_tokens = 1000,\n",
    ")\n",
    "rahane_matches_2023_answer = response.choices[0].text.strip()\n",
    "\n",
    "print(\"Question: How many matches Ruturaj played in 2023?\")\n",
    "print(f\"Answer: {rahane_matches_2023_answer}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fce1a3-c7c7-4179-8743-a3be97291e1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6db62e-7572-4f9f-986f-ba209ca6fafa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Load into Dataframe and restrict to 25 rows and create synthentic text column that would be used for semantic retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3fd6f-c049-4de7-a8ea-f75f4fc2c813",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading data cricket_data.csv that contains cricket player stats data imported from Kaggle\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./cricket_data.csv')\n",
    "\n",
    "# Convert year to string before concatenating and adding space. This is used to create embeddings\n",
    "df[\"text\"] = df[\"Year\"].astype(str) + ' ' + df[\"Player_Name\"]\n",
    "\n",
    "# Restricting rows to 25 to avoid embedding API compute cost.\n",
    "top_25_rows = df.head(25)\n",
    "df = top_25_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9c874-3e9d-4f46-a6e0-380f283437b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the embedding and store it in local file if we have to rerun the script:\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-3-small'))\n",
    "df.to_csv(\"./embeddings.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55763f-9d95-43f6-a8b8-22b46103c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import openai\n",
    "\n",
    "#df = pd.read_csv(\"./embeddings.csv\")\n",
    "#df['embedding'] = df.embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df51cc-3dca-480d-b22e-5d05d5e5fcec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Retrieving rows sorted by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c3755-75a3-4426-b3a6-e43ccffc317b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "  \"\"\"\n",
    "  Function that takes in a question string and a dataframe containing\n",
    "  rows of text and associated embeddings, and returns that dataframe\n",
    "  sorted from least to most relevant for that question\n",
    "  \"\"\"\n",
    "\n",
    "  question_embedding = get_embedding(question)\n",
    "\n",
    "  # Make a copy of the dataframe and add a \"distances\" column containing\n",
    "  # the cosine distances between each row's embeddings and the\n",
    "  # embeddings of the question\n",
    "  df_copy = df.copy()\n",
    "\n",
    "  # Reshape the question embedding to a 2D array with a single row\n",
    "  question_embedding_reshaped = np.expand_dims(question_embedding, axis=0)\n",
    "\n",
    "  # Calculate cosine similarities for each row's embedding with the question embedding\n",
    "  df_copy[\"distances\"] = df_copy[\"embedding\"].apply(\n",
    "      lambda x: cosine_similarity(question_embedding_reshaped, np.array([x]))[0][0]\n",
    "  )\n",
    "\n",
    "  # Sort the copied dataframe by the distances and return it\n",
    "  # (shorter distance = more relevant so we sort in ascending order)\n",
    "  df_copy.sort_values(\"distances\", ascending=False, inplace=True)\n",
    "  return df_copy\n",
    "\n",
    "\n",
    "# Testing the relevance function created above with sample queries:\n",
    "import numpy as np\n",
    "get_rows_sorted_by_relevance(\"How many matches Gaikwad played in 2024\", df)\n",
    "get_rows_sorted_by_relevance(\"What are batting stats for Dhoni in 2024\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4aedc-d82a-4a5a-bfca-69ad1fba07a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Create prompt to create with our custom information to augment with and answering questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a920d-67ec-4b58-b17d-170199e896f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create prompt to create with our custom information to augment with \n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def create_prompt(question, df, max_token_count):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Count the number of tokens in the prompt template and question\n",
    "    prompt_template = \"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context: \n",
    "The below data contains cricket player batting and bowling statistics by year where they played.\n",
    "Using this information answer user questions.\n",
    "\n",
    "{}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "    \n",
    "    context = []\n",
    "\n",
    "    df_sorted = get_rows_sorted_by_relevance(question, df)\n",
    "    for index, row in df_sorted.iterrows():\n",
    "        # Construct the text for the current row\n",
    "        text = f\"Year Played: {row['Year']}, Player Name: {row['Player_Name']}, Matches Batted: {row['Matches_Batted']}, Not Outs: {row['Not_Outs']}, Runs Scored: {row['Runs_Scored']}, Highest Score: {row['Highest_Score']}, Batting Average: {row['Batting_Average']}, Balls Faced: {row['Balls_Faced']}, Batting Strike Rate: {row['Batting_Strike_Rate']}, Centuries: {row['Centuries']}, Half Centuries: {row['Half_Centuries']}, Fours: {row['Fours']}, Sixes: {row['Sixes']}, Catches Taken: {row['Catches_Taken']}, Stumpings: {row['Stumpings']}, Matches Bowled: {row['Matches_Bowled']}, Balls Bowled: {row['Balls_Bowled']}, Runs Conceded: {row['Runs_Conceded']}, Wickets Taken: {row['Wickets_Taken']}, Best Bowling Match: {row['Best_Bowling_Match']}, Bowling Average: {row['Bowling_Average']}, Economy Rate: {row['Economy_Rate']}, Bowling Strike Rate: {row['Bowling_Strike_Rate']}, Four Wicket Hauls: {row['Four_Wicket_Hauls']}, Five Wicket Hauls: {row['Five_Wicket_Hauls']}\\n\"\n",
    "        \n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "        current_token_count += text_token_count\n",
    "        \n",
    "        # Add the row of text to the list if we haven't exceeded the max token count\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
    "\n",
    "# In[ ]:\n",
    "COMPLETION_MODEL_NAME = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=3000, max_answer_tokens=1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response, return the\n",
    "    answer to the question according to an OpenAI Completion model\n",
    "    \n",
    "    If the model produces an error, return an empty string\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = create_prompt(question, df, max_prompt_tokens)\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model = \"gpt-3.5-turbo-instruct\",\n",
    "            prompt = prompt,\n",
    "            max_tokens = max_answer_tokens,\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9460e-aa88-4280-aa8e-2aa465fc36c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Test with RAG Supplied data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78545d-6d2d-44dc-9aaf-0a3f9e1cb00a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testcases: Using RAG supplied data\n",
    "print(\"Using RAG supplied data to answer cricket player stats\")\n",
    "print(\"******************************************************\")\n",
    "\n",
    "# Testcase#1: Testing with a prompt using our custom augmented information\n",
    "custom_answer1 = answer_question(\"What are Dhoni stats in 2023?\", df)\n",
    "print(\"Question: What are Dhoni stats in 2023?\")\n",
    "print(f\"Answer: {custom_answer1}\\n\\n\")\n",
    "\n",
    "# Testcase#2: Testing with a prompt using our custom augmented information\n",
    "custom_answer2 = answer_question(\"How many matches Ruturaj played in 2023\", df)\n",
    "print(\"Question: How many matches Ruturaj played in 2023?\")\n",
    "print(f\"Answer: {custom_answer2}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
